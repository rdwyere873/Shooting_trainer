{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHOOTING TRAINER\n",
    "\n",
    "## Objective:\n",
    "\t\n",
    "    Improve basketball players shooting performance, through the use of computer vision and machine learning.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background:\n",
    "\n",
    "According to Harmon (2016) \"With the rise of deep neural networks, sport prediction experts have new tools for analyzing players, match-ups, and team strategy for unique multiagent systems.(p.2). The recent advancements in computing power, image decomposition, and computer graphics have made it possiblee for players of any age to make substantial improvement in their shooting accuracy and their overall performance on the court, through the use of cutting edge technology. The technology is advancing quickly, already companies like Stats LLC. are currently utilizing high definition cameras positioned in the rafters of NBA stadiums to track every movement that occurs on the court within the game. They then, with the use of image decomposition and object tracking, analyze that data. The end result, ideally, is a predictive model that identifies the attributes of the actions on the court that lead to the highest potential for a positive outcome in any situation. One can only expect these trends to continue as the methods for collecting the data improve tremendously.\n",
    "\n",
    "Over the last 3 years there has been a significant amount of research done regarding the use of video image tracking and professional basketball. Utilizing data gathered by Stats, researchers have been investigating everything from predicting individual’s performance, teams wins and losses, and shot results. This project, is the intial step in a larger project designed by Mark Harmon, Patrick Lucey, and Diego Klabjan in their study entitled “Predicting Shot Making in Basketball Learnt from Adversarial Multiagent Trajectories” (2017). In their research, they predict miss or made shots by looking at factors within the last 5 seconds of the play. They utilize a combination of linear regression, feed forward neural net, and convolutional neural net and measure their results using the log loss function. Their study predicted in-game made or missed shots with a 39% error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Report: \n",
    "\t\n",
    "    \n",
    "The process for building a machine that can improve an individual’s shooting performance is a complex issue, which deals in several different domains. The steps for that process however are the same as any other predictive model. Understand domain or domains, gather some data, preprocess data, feature extraction, fit and train a predictive model, test the results, and gather insights.Successful completion of these tasks will yield a computer machine that can predict the outcome of an individual jump shot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the necessary packages for object tracking\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sys import argv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data:\n",
    "\n",
    "I will be creating my own data in the form of a video of me shooting jumpshot. This gives me the advantage of completely controlling my environmnets, plus I can shoot jump shots for a couple of hours and legitimately call it work. We will be focusing on if the ball is being tracked and why. We will not be focusing on the fact that I nearly shoot an airball becuase I do not bend my knees. \n",
    "\n",
    "The video is in the mp4 format and entitled \"tracking_video\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in video image using opencv capture command\n",
    "\n",
    "cap = cv2.VideoCapture('tracking_video.mp4')\n",
    "\n",
    "# take first frame of the video\n",
    "ret,frame = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup initial location of window\n",
    "r,h,c,w = 250,90,400,125  # simply hardcoded the values\n",
    "track_window = (c,r,w,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up the ROI for tracking by adding the height and width of the object being tracked in every frame\n",
    "roi = frame[r:r+h, c:c+w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opencv perfers the image to be in Hue, Saturation, Value coordinates as opposed to Blue, Green, Red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hsv_roi =  cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)  # convert image color from BGR to HSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an orange mask for the image, allowing the ball to be the main object in the foreground of the image.\n",
    "\n",
    "mask = cv2.inRange(hsv_roi, np.array((9.,100.,100.)), np.array((29.,255.,255.)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Algorithm\n",
    "\n",
    "Once the mask has been established and the ball has been isolated in the forground of the image, the program will process all 2700 frames of the video locating and placing the ball in the ROI in every frame. The result is the tracking of the ball as it moves through the air."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot histogram of the image based on the ditribution of the color orange and min max normalize the  \n",
    "roi_hist = cv2.calcHist([hsv_roi],[0],mask,[180],[0,180])\n",
    "cv2.normalize(roi_hist,roi_hist,0,255,cv2.NORM_MINMAX)\n",
    "\n",
    "# Setup the termination criteria, either 5 iteration or move by atleast 1 pt\n",
    "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 5, 1)\n",
    "\n",
    "#check for end of video if not continue\n",
    "while(1):\n",
    "    ret ,frame = cap.read()\n",
    "\n",
    "    if ret == True:\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)    #caluclate the relative histogram of the image object color\n",
    "\n",
    "        # apply meanshift to get the new location of centroids in each frame\n",
    "        ret, track_window = cv2.CamShift(dst, track_window, term_crit)\n",
    "        \n",
    "    \n",
    "        \n",
    "        # Draw it on image\n",
    "        x,y,w,h = track_window\n",
    "        img2 = cv2.rectangle(frame, (x,y), (x+w,y+h), 255,2)\n",
    "        cv2.imshow('img2',img2)\n",
    "\n",
    "        k = cv2.waitKey(60) & 0xff\n",
    "        if k == 27:\n",
    "\n",
    "            break\n",
    "        else:\n",
    "            cv2.imwrite(chr(k)+\".jpg\",img2)\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
